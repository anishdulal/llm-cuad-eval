# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

"""

# -*- coding: utf-8 -*-
"""
CUAD Long-Context Fine-tuning using Unsloth - Optimized for 65k tokens
Based on SQuAD-style formatting for generative QA tasks
"""

# Installation for Google Colab Pro

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Colab-specific installation - optimized versions
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
#     !pip install transformers==4.51.3
#     !pip install --no-deps unsloth

# # Upgrade datasets for better long-context handling
# !pip install --upgrade datasets fsspec

# Import required libraries
from unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments
from unsloth.chat_templates import train_on_responses_only
import torch
from datasets import load_dataset
from transformers import DataCollatorForLanguageModeling
from unsloth import is_bfloat16_supported
import time
from datetime import datetime
import gc
import json
import numpy as np

print("="*80)
print("CUAD LONG-CONTEXT FINE-TUNING - LLAMA 3.2 3B")
print("Optimized for up to 65k token contexts")
print("="*80)
print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# Model configuration optimized for A100 40GB with long contexts
max_seq_length = 65536  # Full 65k context length
dtype = None  # Auto detection - will use bfloat16 on A100
load_in_4bit = True

print("\n1. Loading Llama 3.2 3B model with long-context support...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)
tokenizer.model_max_length = max_seq_length
tokenizer.init_kwargs["model_max_length"] = max_seq_length

print("2. Adding LoRA adapters - optimized for long-context QA...")
# LoRA configuration following Unsloth best practices
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Balanced rank - good for QA tasks # increase for increasing tuning of more parameters
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj",],
                  #  "embed_tokens", "lm_head"],  # Include embedding layers for better adaptation
    lora_alpha=32,  # 2x the rank as per best practices
    lora_dropout=0,  # Optimized setting per Unsloth
    bias="none",  # Optimized setting per Unsloth
    use_gradient_checkpointing="unsloth",  # Unsloth's memory-efficient version
    random_state=3407,
    use_rslora=True,  # Rank stabilized LoRA for better training
    loftq_config=None,
)

# Explicitly set tokenizer parameters for long context
tokenizer.model_max_length = max_seq_length
tokenizer.pad_token = tokenizer.eos_token
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

print(f"Tokenizer max length set to: {tokenizer.model_max_length}")
print(f"Model max position embeddings: {model.config.max_position_embeddings}")

def preprocess_cuad_function(examples):
    """
    Convert CUAD dataset to SQuAD-style format for generative QA.
    Handles empty answers properly for legal document analysis.
    """
    contexts = examples["context"]
    questions = examples["question"]
    answers = []

    # Process answers - handle the SQuAD-style answer format and empty answers
    for ans in examples["answers"]:
        if ans.get("text") and len(ans["text"]) > 0:
            # Take the first non-empty answer, strip whitespace
            answer_text = next((a.strip() for a in ans["text"] if a.strip()), "")
            # If still empty after stripping, use "Not found"
            answers.append(answer_text if answer_text else "Not found")
        else:
            answers.append("Not found")  # Consistent format for empty answers

    # Create input texts - improved prompt structure
    inputs = [
        f"You are a legal document analyzer. Extract exact phrases from legal documents to answer questions. Only provide the exact text/phrases from the document that answer the question. Do not add explanations or commentary. If the information is not found, respond with 'Not found'.\n\nDocument: {ctx}\n\nQuestion: {q}\n\nAnswer (extract exact phrase from document):"
        for ctx, q in zip(contexts, questions)
    ]

    return {
        "input_text": inputs,
        "target_text": answers,
    }

def formatting_prompts_func(example):
    """
    Format examples for training following Unsloth tutorial pattern.
    This creates the proper training format with end-of-text tokens.
    """
    # Handle both single examples and batched examples
    if isinstance(example["input_text"], str):
        # Single example
        input_text = example["input_text"]
        target_text = example["target_text"]
        # Format: Input + space + target + end-of-text token
        formatted_text = f"{input_text} {target_text}<|eot_id|>"
        return [formatted_text]
    else:
        # Batched examples - return list of formatted strings
        formatted_texts = []
        for inp, tgt in zip(example["input_text"], example["target_text"]):
            formatted_text = f"{inp} {tgt}<|eot_id|>"
            formatted_texts.append(formatted_text)
        return formatted_texts

print("\n3. Loading and preprocessing CUAD dataset...")
try:
    # Load the full CUAD dataset
    cuad_dataset = load_dataset("theatticusproject/cuad-qa", trust_remote_code=True)
    print(f"Loaded {len(cuad_dataset['train'])} training examples")

    # For initial testing, you might want to use a subset
    # Uncomment the next two lines for faster experimentation
    cuad_dataset["train"] = cuad_dataset["train"].shuffle(seed=42).select(range(10000))
    print(f"Using subset of {len(cuad_dataset['train'])} examples for faster training")

except Exception as e:
    print(f"Error loading CUAD dataset: {e}")
    raise

print("4. Preprocessing dataset to SQuAD format...")
try:
    # Apply preprocessing to convert to input_text/target_text format
    cuad_processed = cuad_dataset.map(
        preprocess_cuad_function,
        batched=True,
        num_proc=4,  # Parallel processing
        remove_columns=cuad_dataset["train"].column_names,
        desc="Converting to SQuAD format"
    )

    print(f"Processed dataset size: {len(cuad_processed['train'])} examples")

    # Check for empty examples and filter them out
    def filter_empty_examples(example):
        return (len(example["input_text"].strip()) > 100 and  # Minimum context length
                len(example["target_text"].strip()) > 0)      # Must have some answer (including "Not found")

    cuad_processed["train"] = cuad_processed["train"].filter(filter_empty_examples)
    print(f"After filtering: {len(cuad_processed['train'])} examples")
    train_dataset = cuad_processed["train"]  # Store reference for later use

except Exception as e:
    print(f"Error preprocessing dataset: {e}")
    raise

# Show sample of the processed data
if len(cuad_processed["train"]) > 0:
    print("\nSample processed example:")
    print("-" * 80)
    sample = cuad_processed["train"][0]
    print("Input text (first 500 chars):")
    print(sample["input_text"][:500] + "...")
    print(f"\nTarget text: {sample['target_text']}")
    print("\nFormatted example (first 600 chars):")
    formatted_sample = formatting_prompts_func(sample)[0]
    print(formatted_sample[:600] + "...")
    print("-" * 80)
    print(formatted_sample[-400:])

# Forcefully set the tokenizer's max length. This is still good practice.
tokenizer.model_max_length = max_seq_length
tokenizer.init_kwargs["model_max_length"] = max_seq_length

# This new, single function will handle EVERYTHING: formatting, tokenizing, and masking.
def prepare_and_tokenize_dataset(examples):
    """
    This function takes a batch of examples, formats them into the full prompt-answer
    string, tokenizes them to the full max_seq_length, and creates the masked labels.
    """
    # This is the boundary marker for masking
    answer_prefix = "Answer (extract exact phrase from document):"

    # Format the full text string (prompt + answer + EOS)
    # The `formatting_prompts_func` returns a list, so we access the first element.
    texts = formatting_prompts_func(examples) # This will return a list of formatted strings

    # Tokenize the full texts to the max sequence length

    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding=False, # This is essential!
        max_length=max_seq_length,
    )

    # Create the labels and apply the mask.
    # We will mask everything up to and including the answer_prefix.
    labels = []
    for i in range(len(model_inputs["input_ids"])):
        # Find the start of the answer
        # We search for the tokenized representation of our answer prefix
        # Note: We search in the input_ids of this specific example.

        # We need to find the end of the prompt to mask it.
        # Let's find the start of the answer by finding the answer_prefix tokens
        # We need to tokenize the prefix itself to find its token IDs
        # Note: add_special_tokens=False is important here!
        prefix_tokens = tokenizer.encode(answer_prefix, add_special_tokens=False)

        # Find the sequence of prefix_tokens in the input_ids
        input_ids = model_inputs["input_ids"][i]
        prompt_end_idx = -1
        for k in range(len(input_ids) - len(prefix_tokens)):
            if input_ids[k:k+len(prefix_tokens)] == prefix_tokens:
                prompt_end_idx = k + len(prefix_tokens)
                break

        # Create a copy of the input_ids to use as labels
        label = list(input_ids)

        if prompt_end_idx != -1:
            # Mask everything up to the end of the prefix
            # The +1 is to account for the space token after the prefix
            label[:prompt_end_idx ] = [-100] * (prompt_end_idx )
        else:
            # If for some reason the prefix isn't found, mask the whole sequence
            # This prevents accidental training on prompts if something goes wrong.
            label[:] = [-100] * len(label)

        labels.append(label)

    # Add the labels to our model inputs
    model_inputs["labels"] = labels

    return model_inputs

# --- Apply this new function to the dataset ---
print("Preparing and tokenizing dataset manually to full context length...")
cuad_tokenized = cuad_processed.map(
    prepare_and_tokenize_dataset,
    batched=True,
    # This function is complex, so we run it on a single process.
    num_proc=4,
    remove_columns=cuad_processed["train"].column_names,
    desc="Tokenizing and Masking all examples"
)

# --- Verification Step ---
print("\nVerifying tokenized output...")
print("Input IDs shape of first example:", len(cuad_tokenized['train'][0]['input_ids']))
print("Labels shape of first example:", len(cuad_tokenized['train'][0]['labels']))

print("\n5. Setting up training configuration...")

# Data collator for language modeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Training arguments optimized for long-context and A100 40GB
trainer = UnslothTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=cuad_tokenized["train"],
    # dataset_text_field=None,  # This will be transformed by formatting_func
    # formatting_func=formatting_prompts_func,  # Key function for proper formatting
    max_seq_length=max_seq_length,
    data_collator=data_collator,
    args=UnslothTrainingArguments(
        # Batch size configuration for A100 40GB with 65k context
        per_device_train_batch_size=2,      # Start with 1 due to long sequences
        gradient_accumulation_steps=4,      # Effective batch size = 8

        # Learning rate configuration
        learning_rate=2e-4,                 # Standard Unsloth learning rate
        embedding_learning_rate=1e-5,       # Lower LR for embeddings (continual pretraining)

        # Training schedule
        num_train_epochs=1,                 # Start with 1 epoch for long contexts
        warmup_ratio=0.03,                  # Smaller warmup for long training

        # Optimization settings
        optim="adamw_8bit",                 # Memory-efficient optimizer
        weight_decay=0.01,
        lr_scheduler_type="cosine",         # Cosine scheduler as in reference
        max_grad_norm=1.0,

        # Precision settings for A100
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),

        # Logging and saving
        logging_steps=5,
        save_steps=100,
        save_total_limit=3,

        # Output configuration
        output_dir="./cuad_long_context_outputs",
        report_to="none",

        # Memory optimization
        dataloader_num_workers=2,
        remove_unused_columns=True,        # Keep for custom formatting

        # Reproducibility
        seed=3407,

    ),
)

# Mask everything up through the answer prompt, un-mask only the answer tokens
# trainer = train_on_responses_only(
#     trainer,
#     instruction_part="Answer (extract exact phrase from document):",
#     response_part   ="Answer (extract exact phrase from document):"
# )

# Memory check before training
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"\nGPU: {gpu_stats.name}")
print(f"Max memory: {max_memory} GB")
print(f"Memory reserved before training: {start_gpu_memory} GB")
print(f"Available memory: {max_memory - start_gpu_memory} GB")

# grab a single batch
batch = next(iter(trainer.get_train_dataloader()))
print("Input IDs shape:", batch["input_ids"].shape)
# Expect shape: [batch_size, 65536] not [batch_size, 1024]

# Also inspect one example
feature = trainer.train_dataset[0]
length_after_tokenization = len(feature["input_ids"])
print("Tokenized length of sample 0:", length_after_tokenization)
# Expect: == max_seq_length (or at least > 1024)

# Pick a sample index to inspect (0 for the first example)
idx = 0

# Grab the labels and input_ids for that example
feature = trainer.train_dataset[idx]
labels   = feature["labels"]      # â€“100 for masked tokens, real IDs for un-masked
input_ids= feature["input_ids"]

# Which token IDs survive the masking?
unmasked_ids = [l for l in labels if l != -100]

# Decode them back to text
decoded_answer = tokenizer.decode(unmasked_ids)

print("=== UNMASKED TOKENS (answer) ===")
print(decoded_answer)

# (Optional) see the full prompt for context
full_prompt = tokenizer.decode(input_ids)
print("\n=== FULL PROMPT ===")
print(full_prompt)

print("\nVerifying the masking by decoding the first training example's labels...")

# 1. Grab the first processed example from the trainer's dataset
first_example_processed = trainer.train_dataset[1]

# 2. Get the list of labels. It will be a mix of token IDs and -100s.
labels = first_example_processed["labels"]

# 3. Filter out all the masked tokens (the -100s).
unmasked_label_ids = [label for label in labels if label != -100]

# 4. Decode the remaining token IDs back into a readable string.
decoded_answer = tokenizer.decode(unmasked_label_ids)

print("\n------------------------------------------------------------")
print("Decoded Unmasked Labels (This should ONLY be the answer):")
print(f"'{decoded_answer}'")
print("------------------------------------------------------------\n")

# You can also check the number of unmasked tokens
print(f"Total tokens in sequence: {len(labels)}")
print(f"Number of unmasked (answer) tokens: {len(unmasked_label_ids)}")

# ===================================================================================
# END OF VERIFICATION CODE
# ===================================================================================

# Test the formatting function with a sample
print("\n6. Testing formatting function...")
test_sample = cuad_processed["train"][0]
formatted_test = formatting_prompts_func(test_sample)
print("Formatted training example (first 800 characters):")
print(formatted_test[:800] + "..." if len(formatted_test) > 800 else formatted_test)

print("\n7. Starting training...")
print("=" * 60)
start_time = time.time()

try:
    # Clear cache before training
    torch.cuda.empty_cache()
    gc.collect()

    # Start training
    trainer_stats = trainer.train()

    training_time = time.time() - start_time
    final_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)

    print("=" * 60)
    print("TRAINING COMPLETED SUCCESSFULLY!")
    print(f"Training time: {training_time/60:.2f} minutes")
    print(f"Peak GPU memory: {final_gpu_memory} GB")
    print(f"Memory increase: {final_gpu_memory - start_gpu_memory} GB")

    if trainer.state.log_history:
        final_loss = trainer.state.log_history[-1].get('train_loss', 'N/A')
        print(f"Final training loss: {final_loss}")

    training_successful = True

except Exception as e:
    print(f"Training failed: {e}")
    print("This might be due to memory constraints with very long sequences.")
    print("Consider reducing per_device_train_batch_size to 1 or max_seq_length.")
    training_successful = False
    raise

# Post-training code
if training_successful:
    end_time = time.time()
    training_time = end_time - start_time

    # Final memory stats - FOLLOWING UNSLOTH TUTORIAL FORMAT
    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
    used_percentage = round(used_memory / max_memory * 100, 3)
    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)

    print("=" * 40)
    print("TRAINING COMPLETED SUCCESSFULLY!")
    print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
    print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
    print(f"Peak reserved memory = {used_memory} GB.")
    print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
    print(f"Peak reserved memory % of max memory = {used_percentage} %.")
    print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

    print("\n8. Saving fine-tuned model...")

    # Save LoRA adapters - FOLLOWING UNSLOTH TUTORIAL
    model_save_path = "./cuad_finetuned_llama3_2_3b"
    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)

    # Save training info
    training_info = {
        "model_name": "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
        "dataset": "cuad",
        "training_examples": len(train_dataset),
        "max_seq_length": max_seq_length,
        "training_steps": trainer_stats.metrics.get('train_steps', 'unknown'),
        "training_time_minutes": training_time/60,
        "final_loss": trainer_stats.metrics.get('train_loss', 'unknown'),
        "saved_at": datetime.now().isoformat(),
        "lora_config": {
            "r": 16,  # Updated to match the actual config from first part
            "lora_alpha": 32,  # Updated to match the actual config from first part
            "lora_dropout": 0
        }
    }

    with open(f"{model_save_path}/training_info.json", "w") as f:
        json.dump(training_info, f, indent=2)

    print(f"Model saved to: {model_save_path}")
    print(f"Training info saved to: {model_save_path}/training_info.json")

    tokenizer.pad_token_id = tokenizer.eos_token_id  # or tokenizer.unk_token_id

    print("\n9. Quick inference test...")
    FastLanguageModel.for_inference(model)

    test_context = """
    This Software License Agreement ("Agreement") is entered into on January 1, 2024,
    between Company A and Company B. The term of this Agreement shall be for a period
    of three (3) years from the Effective Date, unless terminated earlier in accordance
    with the terms herein.
    """

    test_question = "What is the duration of this agreement?"

    # Use the same prompt format as in preprocessing
    test_prompt = f"You are a legal document analyzer. Extract exact phrases from legal documents to answer questions. Only provide the exact text/phrases from the document that answer the question. Do not add explanations or commentary. If the information is not found, respond with 'Not found'.\n\nDocument: {test_context}\n\nQuestion: {test_question}\n\nAnswer (extract exact phrase from document):"

    max_new_tokens = 64
    tokenized = tokenizer(
        test_prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_seq_length - max_new_tokens
    ).to("cuda")

    print(f"Test Question: {test_question}")
    print("Model Response:")

    with torch.no_grad():
        outputs = model.generate(
            **tokenized,
            max_new_tokens=max_new_tokens,
            use_cache=True,
            temperature=1.5,
            min_p=0.1,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    # Decode only the generated part (skip the input prompt)
    generated_text = tokenizer.decode(outputs[0][tokenized['input_ids'].shape[1]:], skip_special_tokens=True)
    print(generated_text)

    print("\n" + "="*80)
    print("FINE-TUNING COMPLETED SUCCESSFULLY!")
    print("="*80)
    print(f"Model saved at: {model_save_path}")
    print("You can now use the evaluation script to test performance.")
    print("="*80)

else:
    print("Training failed. Please check the error messages above.")

print(f"\nScript completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)